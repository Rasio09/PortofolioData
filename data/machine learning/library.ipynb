{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2> Kumpulan Library yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy untuk memproses data array / larik untuk operasi matematika (matriks)\n",
    "import numpy as np\n",
    "array = np.array([1, 2, 3, 4])\n",
    "print(array.mean())  # Output: 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas untu memanipulasi dan analisis data terstruktur (filtering, agregasi (mengumpulkan dan mengatur data mentah jadi ringkasan) dan manipulasi)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')# Masukkan alamat penyimpanan file Anda\n",
    "print(df.head())  # Menampilkan 5 baris pertama dari dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib untuk membuat plot atau visualisasi data dalam 2 dimensi (histogram, scatter plot, grafik batang, pie chart)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.histplot(data=df, x=\"column1\") #Pastikan Anda sudah mengimport dataframe\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 1.00\n",
      "\n",
      "Laporan Klasifikasi:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Prediksi kelas untuk sample [[5.0, 3.6, 1.4, 0.2]]: setosa\n"
     ]
    }
   ],
   "source": [
    "# sklearn atau scikit-learn untuk menyediakan alat preprocessing data, evaluasi model dan tuning hyperparameter (klasifikasi, regresi, clustering, dimensionality reduction, dan pemrosesan data)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Fitur\n",
    "y = iris.target  # Label\n",
    "\n",
    "# 2. Bagi dataset menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Buat model Decision Tree\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 4. Latih model dengan data latih\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Lakukan prediksi pada data uji\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluasi model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Akurasi: {accuracy:.2f}\")\n",
    "print(\"\\nLaporan Klasifikasi:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 7. Prediksi untuk satu sample\n",
    "sample = [[5.0, 3.6, 1.4, 0.2]]  # Contoh data bunga\n",
    "predicted_class = model.predict(sample)\n",
    "print(f\"Prediksi kelas untuk sample {sample}: {iris.target_names[predicted_class[0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow untuk ML dapat dipakai untuk deep learning, computer vision, NLP dan reinformcement\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. Normalisasi data\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# 3. One-hot encode label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 4. Bangun model jaringan saraf buatan\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # 10 kelas untuk angka 0-9\n",
    "])\n",
    "\n",
    "# 5. Kompilasi model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 6. Latih model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# 7. Evaluasi model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Akurasi pada data uji: {test_acc:.2f}\")\n",
    "\n",
    "# 8. Prediksi angka pada data uji\n",
    "sample = X_test[0].reshape(1, 28, 28, 1)\n",
    "prediction = model.predict(sample)\n",
    "predicted_class = prediction.argmax()\n",
    "print(f\"Prediksi angka untuk sample pertama: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch sama dengan tensorflow fungsinya\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Persiapan dataset MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalisasi data\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2. Definisi model (Jaringan Saraf Sederhana)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Layer input -> Hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)      # Hidden layer -> Hidden layer\n",
    "        self.fc3 = nn.Linear(64, 10)       # Hidden layer -> Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten gambar\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)          # Output tanpa aktivasi, karena akan menggunakan softmax di loss\n",
    "        return x\n",
    "\n",
    "# 3. Inisialisasi model, loss function, dan optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.CrossEntropyLoss()  # Fungsi loss untuk klasifikasi\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Fungsi untuk melatih model\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            # Reset gradien\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass dan optimisasi\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 5. Fungsi untuk evaluasi model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# 6. Latih dan evaluasi model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# 7. Prediksi untuk satu gambar\n",
    "sample_image, _ = test_dataset[0]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(sample_image.unsqueeze(0))  # Tambahkan dimensi batch\n",
    "    predicted_class = output.argmax().item()\n",
    "    print(f\"Prediksi angka untuk sample pertama: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras library deep learning (prototipe biasanya karena leih simpel)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 2. Preprocessing data\n",
    "# Normalisasi data ke rentang [0, 1] dan tambahkan dimensi untuk channel\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 3. Definisikan model (CNN)\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Output untuk 10 kelas\n",
    "])\n",
    "\n",
    "# 4. Kompilasi model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Latih model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# 6. Evaluasi model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Akurasi pada data uji: {test_acc:.2f}\")\n",
    "\n",
    "# 7. Prediksi angka pada data uji\n",
    "sample = X_test[0].reshape(1, 28, 28, 1)  # Ambil satu sample dan tambahkan batch dimensi\n",
    "prediction = model.predict(sample)\n",
    "predicted_class = prediction.argmax()\n",
    "print(f\"Prediksi angka untuk sample pertama: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK dan Spacy ini untuk NLP untuk tokenizing, stemming (akurasi tidak terlalu penting, index teks, memotong akhiran kata), lemmatization (memerlukan akurasi tinggi, memperhatikan makna, mengubah ke bentuk dasar)\n",
    "\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"better\", \"studies\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)  # Output: ['run', 'fli', 'better', 'studi']\n",
    "\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"better\", \"studies\"]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]  # Pos: v untuk verb\n",
    "print(lemmas)  # Output: ['run', 'fly', 'good', 'study']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "import nltk\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens\n",
    "# ['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[0:6]\n",
    "# [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy untuk melakukan optimisasi, integrasi, dan pemecahan persamaan diferensial serta pengolahan sinyal dan gambar, serta analisis statistik\n",
    "\n",
    "from scipy import optimize\n",
    " \n",
    "def f(x):\n",
    "    return x**2 + 5*x + 6\n",
    " \n",
    "result = optimize.minimize(f, 0)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
